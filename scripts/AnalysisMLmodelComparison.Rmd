---
title: "Analysis ML Model Selection"
author: "Anna Talucci"
date: "2025-05-17"
output: html_document
---


```{r clear environment, include=FALSE}
rm(list=ls())
```

# Overview

ML Model Selection

https://www.tidymodels.org/start/tuning/

**NOTES:**
- â€œSeparation poses no problems for estimating a random forest model. The reason separation is a problem for logistic regression is that the likelihood increases as the absolute value of the coefficient increases. Random forests donâ€™t share this trait because the model isnâ€™t estimated by optimizing an objective function in this manner.â€ [StackExchange link](https://stats.stackexchange.com/questions/441205/doesnt-separation-in-data-affects-random-forest?utm_source=chatgpt.com)
- Ecology application of RF, emphasising very high classification accuracy of RF in ecological datasets: â€œAdvantages of RF â€¦ include (1)  high classification accuracy â€¦â€ (Cutler etâ€¯al.â€¯2007)

# Packages

```{r}

#library(FactoMineR)  # For PCA
#library(factoextra)  # For visualization
library(rpart)
library(rpart.plot)# for visualizing a decision tree

library(tidymodels)  # for the tune package, along with the rest of tidymodels
library(finetune)
# Helper packages
library(rpart.plot)  
library(vip)         # for variable importance plots
library(MASS)
library(randomForest)
library(caret)
library(ranger)
library(xgboost)
library(kernlab)
library(yardstick)
library(rsample)

library(tidyverse)
library(purrr)
library(stringr)
```


# Data

```{r}
allData = read_csv("../outputs/2025-12-05_LC_Site_MultiIndex.csv")
```


# Recode 0/1 response
```{r}
allData
```


```{r}
allData %>% 
   filter(if_any(everything(), is.na))
```

```{r}
( allData1 = allData %>%
    dplyr::select(id, site_ID,  binary, ba_post:vi6t_post, ba_diff:vi6t_diff) %>%
  mutate(tx_num = case_when( # recode response to 0/1
    binary=="burned" ~ 1,
    TRUE ~ 0
  )) %>%
  relocate(id, binary, tx_num) %>%
    drop_na()
)
```

# View Data




# check class balance 

âœ…  Balanced

```{r}
table(allData1$tx_num)
prop.table(table(allData1$tx_num))
```

```{r}
table(allData1$reburn)
prop.table(table(allData1$reburn))
```

# Select Columns for modeling
```{r}
allData1
```

```{r}
( bi_data = allData1 %>% 
    dplyr::select(-binary, -site_ID) %>% 
    mutate(tx_num = as.factor(tx_num)) %>%
    rename(ID=id))

```


# Machine Learning model comparison

```{r}
# Set seed for reproducibility
set.seed(123)

df= bi_data

# Split data
#df_split <- initial_split(df, prop = 0.8, strata = tx_num)
df_split <- group_initial_split(df, group = ID, prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)
# 5-fold cross-validation
#cv_folds <- vfold_cv(df_train, v = 5, strata = tx_num)
#cv_folds <- group_vfold_cv(df_train,group = ID, v = 5) # <- column defining the groups
cv_folds <- group_vfold_cv(df_train, group = ID, v = 5)

# Recipe: normalize only if needed

#base_recipe <- recipe(tx_num ~ ., data = df_train) %>%
  #step_zv(all_predictors()) %>%
  #step_dummy(all_nominal_predictors())
base_recipe <- recipe(tx_num ~ ., data = df_train) %>%
  step_zv(all_predictors()) %>%
  step_novel(ID) %>%
  step_dummy(all_nominal_predictors())
# Model-specific preprocessing
norm_recipe <- base_recipe %>%
  step_normalize(all_numeric_predictors())
# Logistic Regression
log_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
# SVM Linear
svm_linear_spec <- svm_linear(cost = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
# SVM Radial
svm_rbf_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
# Decision Tree
tree_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")
# Random Forest
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
# Gradient Boosted Trees (XGBoost)
xgb_spec <- boost_tree(trees = 500, learn_rate = tune(), mtry = tune(), tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
# Workflows
log_wf <- workflow() %>%
  add_model(log_spec) %>%
  add_recipe(base_recipe)
svm_linear_wf <- workflow() %>%
  add_model(svm_linear_spec) %>%
  add_recipe(norm_recipe)
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(norm_recipe)
tree_wf <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(base_recipe)
rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(base_recipe)
xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(base_recipe)
# Tune each model
#my_metrics <- yardstick::metric_set(yardstick::roc_auc, yardstick::accuracy)
my_metrics <- yardstick::metric_set(
  yardstick::roc_auc,      # discrimination
  yardstick::accuracy,     # overall correctness
  yardstick::recall,
  yardstick::bal_accuracy, # balanced correctness
  yardstick::sens,         # sensitivity (recall)
  yardstick::spec,         # specificity
  yardstick::precision,    # precision
  yardstick::f_meas,       # F1 score
  yardstick::pr_auc,       # PR AUC
  yardstick::brier_class   # calibration metric
)
tune_results <- list(
  log = fit_resamples(log_wf, resamples = cv_folds, metrics = my_metrics, control = control_resamples(save_pred = TRUE)),
  
  svm_linear = tune_grid(svm_linear_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  svm_rbf = tune_grid(svm_rbf_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  tree = tune_grid(tree_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  rf = tune_grid(rf_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  xgb = tune_grid(xgb_wf, resamples = cv_folds, grid = 10, metrics = my_metrics)
)
# Compare models
library(purrr)
results_df <- map_dfr(tune_results, ~collect_metrics(.x), .id = "model")
# Select best model based on highest ROC AUC
best_models <- results_df %>%
  filter(.metric == "roc_auc") %>%
  group_by(model) %>%
  slice_max(mean, n = 1) %>%
  arrange(desc(mean))
print(best_models)
```
```{r eval=FALSE, include=FALSE}
write.csv(best_models, '../outputs/MLselection/2025-12-05_MLSelection.csv', row.names=F)
```

## XGBoost  Model6

```{r}
( xgb_best = results_df %>%
  filter(model=="xgb") %>%
  filter(.config == "Preprocessor1_Model06") %>%
  arrange(desc(mean))
)
```

```{r eval=FALSE, include=FALSE}
write.csv(xgb_best, '../outputs/MLselection/2025-12-05_XGBbest_metrics.csv', row.names=F)
```

# Finalize Model to save as RDS

```{r}
xgb_res <- tune_results$xgb
class(xgb_res)
xgb_metrics <- collect_metrics(xgb_res)

# Check what you have
# View(xgb_metrics) or print(xgb_metrics)

best_xgb_row <- xgb_metrics %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 1)

best_xgb_row

```

```{r}
param_cols <- setdiff(
  names(best_xgb_row),
  c(".metric", "mean", "n", "std_err", ".estimator", ".config")
)

best_xgb_params <- best_xgb_row %>%
  dplyr::select(all_of(param_cols))

best_xgb_params

```

```{r}
final_xgb_wf <- finalize_workflow(xgb_wf, best_xgb_params)

```

```{r}
final_xgb_fit <- final_xgb_wf %>%
  fit(data = df_train)


```

```{r}
extract_fit_parsnip(final_xgb_fit)$fit$levels
```


```{r}
saveRDS(final_xgb_fit, "../model/2025-12-05_final_xgb_binary_model.rds")

```

# Predict on test set 

```{r}
( test_pred <- predict(final_xgb_fit, df_test) %>%
  bind_cols(df_test %>% dplyr::select(tx_num))
)

```

```{r}
cm <- conf_mat(test_pred, truth = tx_num, estimate = .pred_class)
cm

```

```{r}
conf_matrix <- matrix(c(50, 3,
                        0, 45),
                      nrow = 2, byrow = TRUE)


```


```{r}
library(reshape2)

conf_matrix <- matrix(c(50, 3,
                        0, 45),
                      nrow = 2, byrow = TRUE)

# Add class labels
classes <- c("Unburned", "Burned")
rownames(conf_matrix) <- classes  # Predicted
colnames(conf_matrix) <- classes  # Truth

# Convert to data frame for ggplot
conf_df <- melt(conf_matrix)
conf_df
colnames(conf_df) <- c("Predicted", "Truth", "Count")
conf_df$Percentage <- (conf_df$Count / sum(conf_matrix)) * 100
conf_df <- conf_df %>%
  mutate(
    fill_color = case_when(
      Count == 0  ~ "#1F3D3D",
      Count == 3  ~ "#1F3D3D",
      Count == 45 ~ "#127878",
      TRUE        ~ "#E0F0F0"
    )
  )
# Create the heatmap
cf_plot = ggplot(conf_df, aes(x = as.factor(Truth), y = as.factor(Predicted), fill = fill_color)) +
  geom_tile(color = "white", linewidth = 1) +
  #geom_text(aes(label = Count), size = 6, fontface = "bold") +
  geom_text(aes(label = sprintf("%d\n(%.1f%%)", Count, Percentage),
              color = after_scale(ifelse(fill > "#888888",  "black", "white"))), 
          size = 8, fontface = "bold") +
  #scale_fill_viridis_c(name = "Count", option = "rocket") +
   scale_fill_identity() +
  #scale_fill_manual(values = c("#1F3D3D",  "#E0F0F0", "#127878", "#1F3D3D")) +
  #scale_fill_gradient(low = "#fff7ec", high = "#7F0000", name = "Count") +
  #scale_y_discrete(limits = rev(classes)) +  # Reverse y-axis for standard format
  labs(title = "Confusion Matrix",
       x = "Truth",
       y = "Prediction") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 22),
    axis.text = element_text(size=18, face = "bold"),
    axis.text.y = element_text(angle = 90, hjust = 0.5), 
    axis.title = element_text(size=20, face = "bold"),
    panel.grid = element_blank(),
    legend.position = "none"
  ) +
  coord_fixed()  # Keep cells square

cf_plot
# Optional: Save for poster
# ggsave("confusion_matrix.png", width = 8, height = 7, dpi = 300)
```

```{r}
ggsave("../figures/ConfusionMatrixBurnUnburn.png", plot = cf_plot, width = 5, height =5, units = c("in"), dpi=600, bg = "white" )
```

#  **DELETE**
## RF  Model3

```{r}
( rf_best = results_df %>%
  filter(model=="rf") %>%
  filter(.config == "Preprocessor1_Model03") %>%
  arrange(desc(mean))
)
```

```{r eval=FALSE, include=FALSE}
write.csv(rf_best, '../outputs/MLselection/2025-11-06_RFbest_metrics.csv', row.names=F)
```


# Accessing model for prediction

## RF Model

```{r}
rf_best <- select_best(tune_results$rf, metric = "roc_auc") #ðŸ¥‡ 1. Identify the best random forest model
rf_best

( rf_final_wf <- finalize_workflow(rf_wf, rf_best) ) # 2. Finalize the workflow with those parameters

( rf_final_fit <- fit(rf_final_wf, data = df_train) ) # ðŸ§  3. Fit the finalized model on the full training data

```

```{r}
rf_final_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20)

```


```{r}
set.seed(123)
#ðŸ”® 4. Make predictions on test data
( rf_class_preds <- predict(rf_final_fit, df_test) ) 

(rf_prob_preds <- predict(rf_final_fit, df_test, type = "prob") ) # Predicted probabilities (for ROC, PR, etc.):

(rf_preds <- df_test %>%
  bind_cols(rf_class_preds, rf_prob_preds) )

```

```{r}
# ðŸ“ˆ 5. Evaluate on the test set
rf_metrics <- rf_preds %>%
  yardstick::metrics(truth = tx_num, estimate = .pred_class)
rf_metrics

roc_auc(rf_preds, truth = tx_num, .pred_1)
accuracy(rf_preds, truth = tx_num, .pred_class)


```
```{r}
prob_metrics <- yardstick::metric_set(
  roc_auc,
  pr_auc,
  brier_class,
  mn_log_loss
)
prob_metrics(rf_preds, truth = tx_num, .pred_1)
prob_metrics(rf_preds, truth = tx_num, .pred_0)

```

```{r}

class_metrics <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::bal_accuracy,
  yardstick::sens,
  yardstick::spec,        # fully namespaced
  yardstick::precision,   # fully namespaced
  yardstick::f_meas
)


class_metrics(rf_preds, truth = tx_num, estimate = .pred_class)

```

```{r}
conf_mat(rf_preds, truth = tx_num, estimate = .pred_class)
autoplot(conf_mat(rf_preds, truth = tx_num, estimate = .pred_class), type = "heatmap")
```

### ðŸ’¾ 6. (Optional) Save model for later use

```{r eval=FALSE, include=FALSE}
saveRDS(rf_final_fit, "rf_final_fit.rds") # Save
rf_final_fit <- readRDS("rf_final_fit.rds")# reload


```

### ðŸ§© 7. (Optional) Apply to new data

```{r}
AKSample = read_csv('../outputs/cleanedSamplePoints/2025-05-17sampledMultiIndexAK_delta_post.csv')
```

```{r}
set.seed(123)
new_preds <- predict(rf_final_fit, new_df, type = "prob")
```


## XBG Model

```{r}
xgb_best <- select_best(tune_results$xgb, metric = "roc_auc") #ðŸ¥‡ 1. Identify the best XG Boost model
xgb_best

( xgb_final_wf <- finalize_workflow(xgb_wf, xgb_best) ) # 2. Finalize the workflow with those parameters

( xgb_final_fit <- fit(xgb_final_wf, data = df_train) ) # ðŸ§  3. Fit the finalized model on the full training data

```

```{r}
xgb_final_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20)

```

### ðŸ“ˆ predict test set

```{r}
set.seed(123)
#ðŸ”® 4. Make predictions on test data
( xgb_class_preds <- predict(xgb_final_fit, df_test) ) 

(xgb_prob_preds <- predict(xgb_final_fit, df_test, type = "prob") ) # Predicted probabilities (for ROC, PR, etc.):

(xgb_preds <- df_test %>%
  bind_cols(xgb_class_preds, xgb_prob_preds) )

```

### ðŸ“ˆ  Evaluate on the test set
```{r}

xgb_metrics <- xgb_preds %>%
  yardstick::metrics(truth = tx_num, estimate = .pred_class)
xgb_metrics

roc_auc(xgb_preds, truth = tx_num, .pred_1)
accuracy(xgb_preds, truth = tx_num, .pred_class)

```

```{r}
prob_metrics_xgb <- yardstick::metric_set(
  roc_auc,
  pr_auc,
  brier_class,
  mn_log_loss
)
prob_metrics_xgb(xgb_preds, truth = tx_num, .pred_1)
prob_metrics_xgb(xgb_preds, truth = tx_num, .pred_0)

```

```{r}


class_metrics_xgb <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::bal_accuracy,
  yardstick::sens,
  yardstick::spec,        # fully namespaced
  yardstick::precision,   # fully namespaced
  yardstick::f_meas
)


class_metrics_xgb(xgb_preds, truth = tx_num, estimate = .pred_class)

```

```{r}
conf_mat(xgb_preds, truth = tx_num, estimate = .pred_class)
autoplot(conf_mat(xgb_preds, truth = tx_num, estimate = .pred_class), type = "heatmap")
```



### ðŸ’¾ 6. (Optional) Save model for later use

```{r eval=FALSE, include=FALSE}
saveRDS(xgb_final_fit, "xgb_final_fit.rds") # Save
xgb_final_fit <- readRDS("xgb_final_fit.rds")# reload


```

### ðŸ§© 7. (Optional) Apply to new data

```{r}
AKSample = read_csv('../outputs/cleanedSamplePoints/2025-05-17sampledMultiIndexAK_delta_post.csv')
```

```{r}
set.seed(123)
new_preds <- predict(xgb_final_fit, new_df, type = "prob")
```





















## Confustion Matrix

```{r}


# Confusion matrix
conf_mat(rf_preds, truth = tx, estimate = .pred_class)

```
```{r}
conf_mat(rf_preds, truth = tx, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

## Quick variable importance plots

```{r}
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip() +
  ggtitle("All indices (post and delta)")
```

```{r}
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20) + # Top 20 important features
  ggtitle("All indices (post and delta)") 

```


**THE END**