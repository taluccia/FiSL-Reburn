---
title: "Analysis ML Model Selection"
author: "Anna Talucci"
date: "2025-05-17"
output: html_document
---


```{r clear environment, include=FALSE}
rm(list=ls())
```

# Overview

ML Model Selection

https://www.tidymodels.org/start/tuning/

**NOTES:**
- â€œSeparation poses no problems for estimating a random forest model. The reason separation is a problem for logistic regression is that the likelihood increases as the absolute value of the coefficient increases. Random forests donâ€™t share this trait because the model isnâ€™t estimated by optimizing an objective function in this manner.â€ [StackExchange link](https://stats.stackexchange.com/questions/441205/doesnt-separation-in-data-affects-random-forest?utm_source=chatgpt.com)
- Ecology application of RF, emphasising very high classification accuracy of RF in ecological datasets: â€œAdvantages of RF â€¦ include (1)  high classification accuracy â€¦â€ (Cutler etâ€¯al.â€¯2007)

# Packages

```{r}

#library(FactoMineR)  # For PCA
#library(factoextra)  # For visualization
library(rpart)
library(rpart.plot)# for visualizing a decision tree

library(tidymodels)  # for the tune package, along with the rest of tidymodels
library(finetune)
# Helper packages
library(rpart.plot)  
library(vip)         # for variable importance plots
library(MASS)
library(randomForest)
library(caret)
library(ranger)
library(xgboost)
library(kernlab)
library(yardstick)
library(rsample)

library(tidyverse)
library(purrr)
library(stringr)
```


# Data

```{r}
allData = read_csv("../outputs/2025-11-05_LC_Site_MultiIndexForModel.csv")
```


# Recode 0/1 response

```{r}
( allData1 = allData %>%
  mutate(across(where(is.numeric), ~ (.-mean(.))/sd(.))) %>% # scale data
  mutate(tx_num = case_when( # recode response to 0/1
    tx=="burned" ~ 1,
    TRUE ~ 0
  )) %>%
    mutate(reburn_num = case_when( # recode response to 0/1
    reburn=="one" ~ 1,
    reburn=="reburn" ~ 2,
    TRUE ~ 0
  )) %>%
  relocate(ID, tx, tx_num, reburn, reburn_num)
)
```

# View Data

```{r}
allData1
```

```{r}
unique(allData1$tx4)
```

# check class balance 

âœ…  Balanced

```{r}
table(allData1$tx_num)
prop.table(table(allData1$tx_num))
```

```{r}
table(allData1$reburn)
prop.table(table(allData1$reburn))
```

# Select Columns for modeling

```{r}
( bi_data = allData1 %>% dplyr::select(-tx, -tx4, -reburn, -reburn_num) %>% drop_na() %>% mutate(tx_num = as.factor(tx_num)) )
( multi_data = allData1 %>% dplyr::select(-ID, -tx) %>% drop_na() %>% mutate(tx4 = as.factor(tx4)) )
#( deltaData = allData %>% dplyr::select(contains("diff"), tx) %>%  drop_na() %>% mutate(tx = as.factor(tx)) )
#( postData= allData %>% dplyr::select(-row_id) %>% dplyr::select(-contains("diff")) %>% drop_na() %>% mutate(tx = as.factor(tx)) )
```

# Machine Learning model comparison

```{r}
# Set seed for reproducibility
set.seed(123)

df= bi_data

# Split data
#df_split <- initial_split(df, prop = 0.8, strata = tx_num)
df_split <- group_initial_split(df, group = ID, prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)
# 5-fold cross-validation
#cv_folds <- vfold_cv(df_train, v = 5, strata = tx_num)
#cv_folds <- group_vfold_cv(df_train,group = ID, v = 5) # <- column defining the groups
cv_folds <- group_vfold_cv(df_train, group = ID, v = 5)

# Recipe: normalize only if needed

#base_recipe <- recipe(tx_num ~ ., data = df_train) %>%
  #step_zv(all_predictors()) %>%
  #step_dummy(all_nominal_predictors())
base_recipe <- recipe(tx_num ~ ., data = df_train) %>%
  step_zv(all_predictors()) %>%
  step_novel(ID) %>%
  step_dummy(all_nominal_predictors())
# Model-specific preprocessing
norm_recipe <- base_recipe %>%
  step_normalize(all_numeric_predictors())
# Logistic Regression
log_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
# SVM Linear
svm_linear_spec <- svm_linear(cost = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
# SVM Radial
svm_rbf_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
# Decision Tree
tree_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")
# Random Forest
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
# Gradient Boosted Trees (XGBoost)
xgb_spec <- boost_tree(trees = 500, learn_rate = tune(), mtry = tune(), tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
# Workflows
log_wf <- workflow() %>%
  add_model(log_spec) %>%
  add_recipe(base_recipe)
svm_linear_wf <- workflow() %>%
  add_model(svm_linear_spec) %>%
  add_recipe(norm_recipe)
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(norm_recipe)
tree_wf <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(base_recipe)
rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(base_recipe)
xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(base_recipe)
# Tune each model
#my_metrics <- yardstick::metric_set(yardstick::roc_auc, yardstick::accuracy)
my_metrics <- yardstick::metric_set(
  yardstick::roc_auc,      # discrimination
  yardstick::accuracy,     # overall correctness
  yardstick::recall,
  yardstick::bal_accuracy, # balanced correctness
  yardstick::sens,         # sensitivity (recall)
  yardstick::spec,         # specificity
  yardstick::precision,    # precision
  yardstick::f_meas,       # F1 score
  yardstick::pr_auc,       # PR AUC
  yardstick::brier_class   # calibration metric
)
tune_results <- list(
  log = fit_resamples(log_wf, resamples = cv_folds, metrics = my_metrics, control = control_resamples(save_pred = TRUE)),
  
  svm_linear = tune_grid(svm_linear_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  svm_rbf = tune_grid(svm_rbf_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  tree = tune_grid(tree_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  rf = tune_grid(rf_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  xgb = tune_grid(xgb_wf, resamples = cv_folds, grid = 10, metrics = my_metrics)
)
# Compare models
library(purrr)
results_df <- map_dfr(tune_results, ~collect_metrics(.x), .id = "model")
# Select best model based on highest ROC AUC
best_models <- results_df %>%
  filter(.metric == "roc_auc") %>%
  group_by(model) %>%
  slice_max(mean, n = 1) %>%
  arrange(desc(mean))
print(best_models)
```
```{r eval=FALSE, include=FALSE}
write.csv(best_models, '../outputs/MLselection/2025-11-05_MLSelection.csv', row.names=F)
```

## XGBoost  Model2

```{r}
( xgb_best = results_df %>%
  filter(model=="xgb") %>%
  filter(.config == "Preprocessor1_Model02") %>%
  arrange(desc(mean))
)
```

```{r eval=FALSE, include=FALSE}
write.csv(xgb_best, '../outputs/MLselection/2025-11-06_XGBbest_metrics.csv', row.names=F)
```

## RF  Model3

```{r}
( rf_best = results_df %>%
  filter(model=="rf") %>%
  filter(.config == "Preprocessor1_Model03") %>%
  arrange(desc(mean))
)
```

```{r eval=FALSE, include=FALSE}
write.csv(rf_best, '../outputs/MLselection/2025-11-06_RFbest_metrics.csv', row.names=F)
```


# Accessing model for prediction

## RF Model

```{r}
rf_best <- select_best(tune_results$rf, metric = "roc_auc") #ðŸ¥‡ 1. Identify the best random forest model
rf_best

( rf_final_wf <- finalize_workflow(rf_wf, rf_best) ) # 2. Finalize the workflow with those parameters

( rf_final_fit <- fit(rf_final_wf, data = df_train) ) # ðŸ§  3. Fit the finalized model on the full training data

```


```{r}
set.seed(123)
#ðŸ”® 4. Make predictions on test data
( rf_class_preds <- predict(rf_final_fit, df_test) ) 

(rf_prob_preds <- predict(rf_final_fit, df_test, type = "prob") ) # Predicted probabilities (for ROC, PR, etc.):

(rf_preds <- df_test %>%
  bind_cols(rf_class_preds, rf_prob_preds) )

```

```{r}
# ðŸ“ˆ 5. Evaluate on the test set
rf_metrics <- rf_preds %>%
  yardstick::metrics(truth = tx_num, estimate = .pred_class)
rf_metrics

roc_auc(rf_preds, truth = tx_num, .pred_1)
accuracy(rf_preds, truth = tx_num, .pred_class)


```
```{r}
prob_metrics <- yardstick::metric_set(
  roc_auc,
  pr_auc,
  brier_class,
  mn_log_loss
)
prob_metrics(rf_preds, truth = tx_num, .pred_1)
prob_metrics(rf_preds, truth = tx_num, .pred_0)

```

```{r}

class_metrics <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::bal_accuracy,
  yardstick::sens,
  yardstick::spec,        # fully namespaced
  yardstick::precision,   # fully namespaced
  yardstick::f_meas
)


class_metrics(rf_preds, truth = tx_num, estimate = .pred_class)

```

```{r}
conf_mat(rf_preds, truth = tx_num, estimate = .pred_class)
autoplot(conf_mat(rf_preds, truth = tx_num, estimate = .pred_class), type = "heatmap")
```

### ðŸ’¾ 6. (Optional) Save model for later use

```{r eval=FALSE, include=FALSE}
saveRDS(rf_final_fit, "rf_final_fit.rds") # Save
rf_final_fit <- readRDS("rf_final_fit.rds")# reload


```

### ðŸ§© 7. (Optional) Apply to new data

```{r}
AKSample = read_csv('../outputs/cleanedSamplePoints/2025-05-17sampledMultiIndexAK_delta_post.csv')
```

```{r}
set.seed(123)
new_preds <- predict(rf_final_fit, new_df, type = "prob")
```


## XBG Model

```{r}
xgb_best <- select_best(tune_results$xgb, metric = "roc_auc") #ðŸ¥‡ 1. Identify the best XG Boost model
xgb_best

( xgb_final_wf <- finalize_workflow(xgb_wf, xgb_best) ) # 2. Finalize the workflow with those parameters

( xgb_final_fit <- fit(xgb_final_wf, data = df_train) ) # ðŸ§  3. Fit the finalized model on the full training data

```


```{r}
set.seed(123)
#ðŸ”® 4. Make predictions on test data
( xgb_class_preds <- predict(xgb_final_fit, df_test) ) 

(xgb_prob_preds <- predict(xgb_final_fit, df_test, type = "prob") ) # Predicted probabilities (for ROC, PR, etc.):

(xgb_preds <- df_test %>%
  bind_cols(xgb_class_preds, xgb_prob_preds) )

```

```{r}
# ðŸ“ˆ 5. Evaluate on the test set
xgb_metrics <- xgb_preds %>%
  yardstick::metrics(truth = tx_num, estimate = .pred_class)
xgb_metrics

roc_auc(xgb_preds, truth = tx_num, .pred_1)
accuracy(xgb_preds, truth = tx_num, .pred_class)

```

```{r}
prob_metrics_xgb <- yardstick::metric_set(
  roc_auc,
  pr_auc,
  brier_class,
  mn_log_loss
)
prob_metrics_xgb(xgb_preds, truth = tx_num, .pred_1)
prob_metrics_xgb(xgb_preds, truth = tx_num, .pred_0)

```

```{r}


class_metrics_xgb <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::bal_accuracy,
  yardstick::sens,
  yardstick::spec,        # fully namespaced
  yardstick::precision,   # fully namespaced
  yardstick::f_meas
)


class_metrics_xgb(xgb_preds, truth = tx_num, estimate = .pred_class)

```

```{r}
conf_mat(rf_preds, truth = tx_num, estimate = .pred_class)
autoplot(conf_mat(rf_preds, truth = tx_num, estimate = .pred_class), type = "heatmap")
```

### ðŸ’¾ 6. (Optional) Save model for later use

```{r eval=FALSE, include=FALSE}
saveRDS(xgb_final_fit, "xgb_final_fit.rds") # Save
xgb_final_fit <- readRDS("xgb_final_fit.rds")# reload


```

### ðŸ§© 7. (Optional) Apply to new data

```{r}
AKSample = read_csv('../outputs/cleanedSamplePoints/2025-05-17sampledMultiIndexAK_delta_post.csv')
```

```{r}
set.seed(123)
new_preds <- predict(xgb_final_fit, new_df, type = "prob")
```





















## Confustion Matrix

```{r}


# Confusion matrix
conf_mat(rf_preds, truth = tx, estimate = .pred_class)

```
```{r}
conf_mat(rf_preds, truth = tx, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

## Quick variable importance plots

```{r}
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip() +
  ggtitle("All indices (post and delta)")
```

```{r}
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20) + # Top 20 important features
  ggtitle("All indices (post and delta)") 

```


**THE END**