---
title: "Analysis ML Model Selection"
author: "Anna Talucci"
date: "2025-05-17"
output: html_document
---


```{r clear environment, include=FALSE}
rm(list=ls())
```

# Overview

ML Model Selection

https://www.tidymodels.org/start/tuning/

**NOTES:**
- “Separation poses no problems for estimating a random forest model. The reason separation is a problem for logistic regression is that the likelihood increases as the absolute value of the coefficient increases. Random forests don’t share this trait because the model isn’t estimated by optimizing an objective function in this manner.” [StackExchange link](https://stats.stackexchange.com/questions/441205/doesnt-separation-in-data-affects-random-forest?utm_source=chatgpt.com)
- Ecology application of RF, emphasising very high classification accuracy of RF in ecological datasets: “Advantages of RF … include (1)  high classification accuracy …” (Cutler et al. 2007)

# Packages

```{r}

#library(FactoMineR)  # For PCA
#library(factoextra)  # For visualization
library(rpart)
library(rpart.plot)# for visualizing a decision tree

library(tidymodels)  # for the tune package, along with the rest of tidymodels
library(finetune)
# Helper packages
library(rpart.plot)  
library(vip)         # for variable importance plots
library(MASS)
library(randomForest)
library(caret)
library(ranger)
library(xgboost)
library(kernlab)
library(yardstick)
library(rsample)

library(tidyverse)
library(purrr)
library(stringr)
```


# Data

```{r}
allData = read_csv("../outputs/2025-11-05_LC_Site_MultiIndexForModel.csv")
```


# Recode 0/1 response

```{r}
( allData1 = allData %>%
  mutate(across(where(is.numeric), ~ (.-mean(.))/sd(.))) %>% # scale data
  mutate(tx_num = case_when( # recode response to 0/1
    tx=="burned" ~ 1,
    TRUE ~ 0
  )) %>%
    mutate(reburn_num = case_when( # recode response to 0/1
    reburn=="one" ~ 1,
    reburn=="reburn" ~ 2,
    TRUE ~ 0
  )) %>%
  relocate(ID, tx, tx_num, reburn, reburn_num)
)
```

# View Data

```{r}
allData1
```

```{r}
unique(allData1$tx4)
```

# check class balance 

✅  Balanced

```{r}
table(allData1$tx_num)
prop.table(table(allData1$tx_num))
```

```{r}
table(allData1$reburn)
prop.table(table(allData1$reburn))
```

# Select Columns for modeling

```{r}
( bi_data = allData1 %>% dplyr::select(-tx, -tx4, -reburn, -reburn_num) %>% drop_na() %>% mutate(tx_num = as.factor(tx_num)) )
( multi_data = allData1 %>% dplyr::select(-ID, -tx) %>% drop_na() %>% mutate(tx4 = as.factor(tx4)) )
#( deltaData = allData %>% dplyr::select(contains("diff"), tx) %>%  drop_na() %>% mutate(tx = as.factor(tx)) )
#( postData= allData %>% dplyr::select(-row_id) %>% dplyr::select(-contains("diff")) %>% drop_na() %>% mutate(tx = as.factor(tx)) )
```

# Machine Learning model comparison

```{r}
# Set seed for reproducibility
set.seed(123)

df= bi_data

# Split data
#df_split <- initial_split(df, prop = 0.8, strata = tx_num)
df_split <- group_initial_split(df, group = ID, prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)
# 5-fold cross-validation
#cv_folds <- vfold_cv(df_train, v = 5, strata = tx_num)
#cv_folds <- group_vfold_cv(df_train,group = ID, v = 5) # <- column defining the groups
cv_folds <- group_vfold_cv(df_train, group = ID, v = 5)

# Recipe: normalize only if needed

#base_recipe <- recipe(tx_num ~ ., data = df_train) %>%
  #step_zv(all_predictors()) %>%
  #step_dummy(all_nominal_predictors())
base_recipe <- recipe(tx_num ~ ., data = df_train) %>%
  step_zv(all_predictors()) %>%
  step_novel(ID) %>%
  step_dummy(all_nominal_predictors())
# Model-specific preprocessing
norm_recipe <- base_recipe %>%
  step_normalize(all_numeric_predictors())
# Logistic Regression
log_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
# SVM Linear
svm_linear_spec <- svm_linear(cost = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
# SVM Radial
svm_rbf_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
# Decision Tree
tree_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")
# Random Forest
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
# Gradient Boosted Trees (XGBoost)
xgb_spec <- boost_tree(trees = 500, learn_rate = tune(), mtry = tune(), tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
# Workflows
log_wf <- workflow() %>%
  add_model(log_spec) %>%
  add_recipe(base_recipe)
svm_linear_wf <- workflow() %>%
  add_model(svm_linear_spec) %>%
  add_recipe(norm_recipe)
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(norm_recipe)
tree_wf <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(base_recipe)
rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(base_recipe)
xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(base_recipe)
# Tune each model
#my_metrics <- yardstick::metric_set(yardstick::roc_auc, yardstick::accuracy)
my_metrics <- yardstick::metric_set(
  yardstick::roc_auc,      # discrimination
  yardstick::accuracy,     # overall correctness
  yardstick::recall,
  yardstick::bal_accuracy, # balanced correctness
  yardstick::sens,         # sensitivity (recall)
  yardstick::spec,         # specificity
  yardstick::precision,    # precision
  yardstick::f_meas,       # F1 score
  yardstick::pr_auc,       # PR AUC
  yardstick::brier_class   # calibration metric
)
tune_results <- list(
  log = fit_resamples(log_wf, resamples = cv_folds, metrics = my_metrics, control = control_resamples(save_pred = TRUE)),
  
  svm_linear = tune_grid(svm_linear_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  svm_rbf = tune_grid(svm_rbf_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  tree = tune_grid(tree_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  rf = tune_grid(rf_wf, resamples = cv_folds, grid = 10, metrics = my_metrics),
  
  xgb = tune_grid(xgb_wf, resamples = cv_folds, grid = 10, metrics = my_metrics)
)
# Compare models
library(purrr)
results_df <- map_dfr(tune_results, ~collect_metrics(.x), .id = "model")
# Select best model based on highest ROC AUC
best_models <- results_df %>%
  filter(.metric == "roc_auc") %>%
  group_by(model) %>%
  slice_max(mean, n = 1) %>%
  arrange(desc(mean))
print(best_models)
```
```{r eval=FALSE, include=FALSE}
write.csv(best_models, '../outputs/MLselection/2025-11-05_MLSelection.csv', row.names=F)
```

## XGBoost  Model2

```{r}
( xgb_best = results_df %>%
  filter(model=="xgb") %>%
  filter(.config == "Preprocessor1_Model02") %>%
  arrange(desc(mean))
)
```

```{r eval=FALSE, include=FALSE}
write.csv(xgb_best, '../outputs/MLselection/2025-11-06_XGBbest_metrics.csv', row.names=F)
```

## RF  Model3

```{r}
( rf_best = results_df %>%
  filter(model=="rf") %>%
  filter(.config == "Preprocessor1_Model03") %>%
  arrange(desc(mean))
)
```

```{r eval=FALSE, include=FALSE}
write.csv(rf_best, '../outputs/MLselection/2025-11-06_RFbest_metrics.csv', row.names=F)
```


# Working with RF model

## Step 1: Get best parameters for the random forest 

```{r}
# Extract random forest tuning results
rf_tuned <- tune_results$rf

# Get best parameters based on ROC AUC
rf_best <- select_best(rf_tuned, metric = "roc_auc")

rf_best

```

## Step 2: Finalize the workflow with those parameters

```{r}
( rf_final_wf <- finalize_workflow(rf_wf, rf_best) )

```

## Step 3:

```{r}
( rf_final_fit <- fit(rf_final_wf, data = df_train) )

```

## Step 4: Calibration or prediction check

```{r}
( rf_class_preds <- predict(rf_final_fit, df_test) )
```


## Step 5: Access the underlying ranger model

```{r}
( rf_model_object <- extract_fit_parsnip(rf_final_fit)$fit )

```

```{r}
rf_model_object$variable.importance

```

# Evaluate calibration and confidence

## Step 1: Generate predicted probabilities

```{r}

# Predict probabilities for class "1"
( rf_preds <- predict(rf_final_fit, df_test, type = "prob") %>%
  bind_cols(df_test %>% dplyr::select(tx_num)) %>%
  rename(prob_0 = .pred_0, prob_1 = .pred_1)
)

```

## Step 2: Visualize predicted probability

```{r}


ggplot(rf_preds, aes(x = prob_1, fill = factor(tx_num))) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(fill = "Observed class",
       x = "Predicted probability (class unburned)",
       y = "Count",
       title = "Random Forest: Predicted Probability Distribution") +
  theme_minimal()

ggplot(rf_preds, aes(x = prob_0, fill = factor(tx_num))) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(fill = "Observed class",
       x = "Predicted probability (class burned)",
       y = "Count",
       title = "Random Forest: Predicted Probability Distribution") +
  theme_minimal()


```

Note:
-- If most predictions cluster near 0 or 1 → potential overconfidence.
-- If probabilities are more evenly spread → model is more uncertain and often better calibrated.

## Step 3: reliability plots

```{r}
rf_cal <- rf_preds %>%
  mutate(bin = cut(prob_1, breaks = seq(0, 1, 0.1))) %>%
  group_by(bin) %>%
  summarise(
    mean_pred = mean(prob_1),
    obs_rate = mean(as.numeric(tx_num) == 1),
    .groups = "drop"
  )

ggplot(rf_cal, aes(x = mean_pred, y = obs_rate)) +
  geom_point(size = 3, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  coord_equal() +
  labs(
    title = "Calibration Plot (Random Forest)",
    x = "Mean predicted probability",
    y = "Observed proportion"
  ) +
  theme_minimal()

```

Notes:
-- Points along the 45° line → good calibration
-- Below line → overconfident (predicts higher probabilities than reality)
-- Above line → underconfident


## Step 4: Brier score for calibration

```{r}
brier_score <- mean((rf_preds$prob_1 - as.numeric(df_test$tx_num))^2)
brier_score
```
Notes: 
-- Range: 0 (perfect) → 0.25 (bad for binary 50/50 random guessing)
-- Lower is better


## Step 5: ROC AUC

```{r}
library(pROC)

roc_obj <- roc(df_test$tx_num, rf_preds$prob_1)
plot(roc_obj, col = "steelblue", main = "ROC Curve (Random Forest)")
auc(roc_obj)

```

Notes:
-- High AUC (≥ 0.8) → model distinguishes classes well
-- But still check calibration — high AUC ≠ well-calibrated

## Step 6: quanititive calibration with yardstick

```{r}
(rf_preds_tidy <- rf_preds %>%
  mutate(.pred_class = ifelse(prob_1 > 0.5, "1", "0")) %>%
  mutate(.pred_class = factor(.pred_class, levels = c("0", "1")))
)

metrics(rf_preds_tidy, truth = tx_num, estimate = .pred_class)
roc_auc(rf_preds_tidy, truth = tx_num, prob_1)
roc_auc(rf_preds_tidy, truth = tx_num, prob_0)

```
Interpration
| Metric / Plot         | What It Tells You        | What to Watch For                   |
| --------------------- | ------------------------ | ----------------------------------- |
| Probability histogram | Overall model confidence | Extreme 0/1 spikes → overconfidence |
| Calibration plot      | Probability reliability  | Points below line → overconfident   |
| Brier score           | Summary of calibration   | Lower = better                      |
| ROC AUC               | Ranking quality          | Higher = better                     |


# Recalibration

Notes: 
Random forests often produce overconfident probabilities, especially with class imbalance or strong predictors (like your case with variable separation).
Calibration fits a secondary model that “reshapes” those probabilities to better match observed frequencies.

## Step 1: create calibrated data set

```{r}


# Get predicted probabilities
rf_probs <- predict(rf_final_fit, df_test, type = "prob") %>%
  bind_cols(df_test %>% dplyr::select(tx_num)) %>%
  rename(prob_0 = .pred_0, prob_1 = .pred_1)

```

## Step 2: Split for calibration

```{r}
cal_data <- rf_probs %>%
  dplyr::select(tx_num, prob_1)

```

## Step 3a: Platt Calibration

```{r}
platt_calibrator <- glm(tx_num ~ prob_1, data = cal_data, family = binomial)

```

```{r}

( rf_probs <- rf_probs %>%
  mutate(prob_1_platt = predict(platt_calibrator, newdata = cal_data, type = "response")) )

```

## Step 3b: Isotonic regression

```{r}
library(isotone)

# Fit isotonic regression
iso_model <- isoreg(cal_data$prob_1, as.numeric(cal_data$tx_num))

# Manually interpolate predictions
rf_probs$prob_1_iso <- with(iso_model, 
  pmin(pmax(approx(x, yf, xout = cal_data$prob_1, ties = "ordered")$y, 0), 1)
)

```

```{r}


rf_long <- rf_probs %>%
  dplyr::select(tx_num, raw = prob_1, Platt = prob_1_platt, Isotonic = prob_1_iso) %>%
  pivot_longer(-tx_num, names_to = "method", values_to = "prob")

rf_cal <- rf_long %>%
  mutate(bin = cut(prob, breaks = seq(0, 1, 0.1))) %>%
  group_by(method, bin) %>%
  summarise(mean_pred = mean(prob), obs_rate = mean(as.numeric(tx_num) == 1), .groups = "drop")

ggplot(rf_cal, aes(x = mean_pred, y = obs_rate, color = method)) +
  geom_point(size = 3, position = "dodge") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  coord_equal() +
  theme_minimal() +
  labs(title = "Calibration Curves: Raw vs Platt vs Isotonic",
       x = "Mean predicted probability",
       y = "Observed proportion")

```


## Step 5: Brier

```{r}
library(pROC)


# Ensure response is numeric 0/1
rf_probs <- rf_probs %>%
  mutate(tx_num = as.numeric(tx_num == 1))

# Function to calculate Brier score
brier_score <- function(pred, obs) mean((pred - obs)^2)

# Compute metrics for each method
metrics_df <- tibble(
  Method = c("Raw", "Platt", "Isotonic"),
  Brier = c(
    brier_score(rf_probs$prob_1, rf_probs$tx_num),
    brier_score(rf_probs$prob_1_platt, rf_probs$tx_num),
    brier_score(rf_probs$prob_1_iso, rf_probs$tx_num)
  ),
  AUC = c(
    auc(roc(rf_probs$tx_num, rf_probs$prob_1)),
    auc(roc(rf_probs$tx_num, rf_probs$prob_1_platt)),
    auc(roc(rf_probs$tx_num, rf_probs$prob_1_iso))
  )
)

metrics_df


```



---------------

```{r}
# Check performance on a test set
set.seed(123)
data= df_scaled
split <- initial_split(data, prop = 0.8)
train <- training(split)
test <- testing(split)

rf_fit <- rf_spec %>% fit(tx ~ ., data = train)

preds <- predict(rf_fit, test, type = "prob") %>%
  bind_cols(predict(rf_fit, test)) %>%
  bind_cols(test)

roc_auc(preds, truth = tx, .pred_burned)

```

```{r}
# Check performance on a test set
set.seed(123)
data= df_scaled
split <- initial_split(data, prop = 0.8)
train <- training(split)
test <- testing(split)

rf_fit <- rf_spec %>% fit(tx ~ ., data = train)

preds <- predict(rf_fit, test, type = "prob") %>%
  bind_cols(predict(rf_fit, test)) %>%
  bind_cols(test)

roc_auc(preds, truth = tx, .pred_unburned)

```

```{r}
set.seed(123)
data= df_scaled

cv <- vfold_cv(data, v = 10, repeats = 5)

rf_tuned <- rf_wf %>%
  tune_grid(
    resamples = cv,
    grid = 20,   # or a defined grid object
    metrics = metric_set(roc_auc, accuracy)
  )

collect_metrics(rf_tuned)


```
nbrt1_post + csi_post + mirbi_post + nbr_post + vi45_post + ndmi_post + nbrt1_diff + swir2_diff + thermal_diff
```{r}
names(df_scaled)
```


# Predict classes for sampled points

```{r}
stratSample = read_csv('../outputs/cleanedSamplePoints/2025-05-17sampledMultiIndexAK_delta_post.csv')
```

```{r}
stratSample
```
```{r}
(stratSampleTx = stratSample %>% 
   drop_na() %>%
      dplyr::select(row_id, tx) 
 )
```

```{r}
(stratSampleSub = stratSample %>% 
   dplyr::select(-tx) %>%
   drop_na() %>%
  mutate(across(where(is.numeric) & !row_id, ~ (.-mean(.))/sd(.))) %>%
  rename_with(~ paste0(., "_post"), .cols = 2:24)
)
```

```{r}
set.seed(123)
predicted_classes <- predict(rf_fit, stratSampleSub)
predicted_classes
```

```{r}
results <- cbind(stratSampleSub, Predicted = predicted_classes)
head(results)
names
```

```{r}
stratSampleTx %>% full_join(results, by="row_id") %>% rename(pred_class = ".pred_class") %>% relocate(row_id, tx, pred_class)
```

## Logistic Regression

```{r}
# Example: Dependent variable 'outcome' predicted by 'predictor1' and 'predictor2'
model1 <- glm(tx ~ nbrt1_post + csi_post + mirbi_post + nbr_post + vi45_post + ndmi_post + nbrt1_diff + swir2_diff + thermal_diff, data = df_scaled, family = binomial)
```

```{r}
summary(model1)
```


```{r}

ctrl <- rfeControl(functions = lrFuncs, method = "cv", number = 10)

set.seed(123)
rfe_fit <- rfe(
  x = df_scaled[, setdiff(names(df_scaled), "tx")],
  y = df_scaled$tx,
  sizes = c(1:10),
  rfeControl = ctrl
)

rfe_fit
predictors(rfe_fit)


```

### Check for separation
```{r}
install.packages("brglm2")
library(brglm2)

detect_separation(tx ~ ., data = df)

```

```{r}


ggplot(df_scaled, aes(x = tx , y = ndmi_post, color = tx)) +
  geom_jitter(height = 0.1) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))

```

```{r}

plot_numeric_predictors <- function(df, response) {
  # Identify numeric predictor variables (excluding the response column)
  numeric_vars <- names(df)[sapply(df, is.numeric) & names(df) != response]
  
  # Prepare data with numeric and factor response
  df <- df %>%
    mutate(
      resp_factor = as.factor(.data[[response]]),  # for color
      resp_num = as.numeric(.data[[response]])     # for glm
    )
  
  # Generate a plot for each numeric predictor
  plots <- lapply(numeric_vars, function(var) {
    ggplot(df, aes_string(x = var, y = "resp_num", color = "resp_factor")) +
      geom_jitter(height = 0.1) +
      geom_smooth(method = "glm", method.args = list(family = "binomial")) +
      labs(x = var, y = response, color = response)
  })
  
  names(plots) <- numeric_vars
  return(plots)
}


```

```{r}
plots <- plot_numeric_predictors(df_scaled, "tx")

# View one plot
plots$ndmi_post

# View all
for (p in plots) print(p)

```

ndmi_post + vi45_post + csi_post + red_post + vi43_diff + csi_diff + ndmi_diff + vi57_diff + nbrt1_diff + green_post + nbr2_diff

```{r}
library(glmnet)

x <- model.matrix(tx ~ . - 1, data = df_scaled)
y <- df_scaled$tx

set.seed(123)
cv_fit <- cv.glmnet(x, y, family = "binomial", alpha = 1) # LASSO

plot(cv_fit)
coef(cv_fit, s = "lambda.min")

```

```{r}
cor_matrix <- cor(df_scaled %>% dplyr::select(where(is.numeric)))
findCorrelation(cor_matrix, cutoff = 0.9)

```

```{r}
cv_fit$glmnet.fit

```

```{r}
(cor_matrix <- cor(df_scaled %>% dplyr::select(where(is.numeric)), use = "pairwise.complete.obs") )

```

```{r}
findCorrelation(cor_matrix, cutoff = 0.9)
```

```{r}


high_corr_idx <- findCorrelation(cor_matrix, cutoff = 0.9, names = FALSE)
high_corr_names <- colnames(cor_matrix)[high_corr_idx]
high_corr_names

```

```{r}

library(corrplot)
corrplot(cor_matrix, method = "color", tl.cex = 0.7)
```


## Logistic regression in tidy models
✅ Advantages of this approach

Handles small sample size or perfect separation

Automatically scales numeric variables

Uses cross-validation to pick the best penalty

Provides sparse coefficients → easier to interpret

Fully integrates with the tidymodels workflow

```{r}
# Define your data
set.seed(123)
split <- initial_split(df, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)

# define recipe and standarize data
rec <- recipe(tx ~ ., data = train_data) %>%  # replace tx with your outcome
  step_normalize(all_numeric_predictors())    # scales all numeric columns

# Define a LASSO logistic regression model
lasso_spec <- logistic_reg(
  penalty = tune(),  # LASSO regularization strength
  mixture = 1        # mixture = 1 → pure LASSO, 0 → Ridge
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Combine recipe + model into a workflow
lasso_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lasso_spec)

# cross validation
cv_splits <- vfold_cv(train_data, v = 10)

# tune
# Create a grid of penalty values (can be log-spaced)
penalty_grid <- grid_regular(penalty(range = c(-4, 0)), levels = 50)

lasso_tune <- tune_grid(
  lasso_wf,
  resamples = cv_splits,
  grid = penalty_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# See best penalty
best_penalty <- select_best(lasso_tune, metric = "roc_auc")

best_penalty

# Finalize workflow
final_wf <- finalize_workflow(lasso_wf, best_penalty)

# Fit on full training data
final_fit <- fit(final_wf, data = train_data)

# use on test set
preds <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc(preds, truth = tx, .pred_burned)  # replace .pred_1 with your positive class


# extract selected predictors
final_fit %>%
  pull_workflow_fit() %>%
  tidy() %>%
  filter(estimate != 0)

```

# Mutinomial Model

```{r}
set.seed(123)
df = multi_data
# Make sure outcome is a factor
df$tx3 <- factor(df$tx3)

# Split data
split <- initial_split(df, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)

# Recipe
rec <- recipe(tx3 ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Multinomial logistic regression with glmnet
multi_spec <- multinom_reg(
  penalty = tune(),  # optional for LASSO
  mixture = 1        # LASSO
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Workflow
multi_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(multi_spec)

# Cross-validation
cv_splits <- vfold_cv(train_data, v = 5)

# Tuning penalty (optional)
grid <- grid_regular(penalty(range = c(-4, 0)), levels = 20)
multi_tune <- tune_grid(
  multi_wf,
  resamples = cv_splits,
  grid = grid,
  metrics = metric_set(roc_auc, accuracy)
)

# Select best lambda
best_penalty <- select_best(multi_tune, metric = "roc_auc")
best_penalty
# Finalize workflow
final_wf <- finalize_workflow(multi_wf, best_penalty)
final_fit <- fit(final_wf, data = train_data)

# Predict probabilities on test set
predict(final_fit, test_data, type = "prob")

preds <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)
preds

# Predicted classes as a factor
pred_class <- predict(final_fit, test_data, type = "class")$.pred_class

# Compute accuracy
accuracy_vec(truth = test_data$tx3, estimate = pred_class)

#prob_cols <- names(pred_probs)[startsWith(names(pred_probs), ".pred_")]

#roc_auc(
 # pred_probs,
  #truth = y,
  #all_of(prob_cols),
  #estimator = "macro"
#)

final_fit %>%
  pull_workflow_fit() %>%
  tidy() %>%
  filter(estimate != 0)

```

# Fun RF model

```{r}
# Extract best parameters for random forest based on ROC AUC
best_rf_params <- select_best(tune_results$rf, metric = "roc_auc")
print(best_rf_params)

```

```{r}
# Finalize the rf workflow
final_rf_wf <- finalize_workflow(rf_wf, best_rf_params)

```

```{r}
# Fit the finalized random forest model
final_rf_fit <- fit(final_rf_wf, data = df_train)
final_rf_fit
```


```{r}
# Predict on test data
( rf_preds <- predict(final_rf_fit, df_test, type = "prob") %>%
  bind_cols(predict(final_rf_fit, df_test)) %>%
  bind_cols(df_test)
)
```
```{r}
roc_auc(rf_preds, truth = tx, .pred_burned)
```

```{r}
# Evaluate performance
library(yardstick)
rf_metrics <- rf_preds %>%
  metrics(truth = tx, estimate = .pred_class, .pred_burned) %>%  # change `.pred_1` to your positive class
  bind_rows(roc_auc(rf_preds, truth = tx, .pred_burned))          # change `.pred_1` as needed
print(rf_metrics)

```

## Confustion Matrix

```{r}


# Confusion matrix
conf_mat(rf_preds, truth = tx, estimate = .pred_class)

```
```{r}
conf_mat(rf_preds, truth = tx, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

##

```{r}
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip() +
  ggtitle("All indices (post and delta)")
```

```{r}
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20) + # Top 20 important features
  ggtitle("All indices (post and delta)") 

```


**THE END**